{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_Wk3_SGDRegressor.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression with Iterative optimization: SGDRegressor"
      ],
      "metadata": {
        "id": "NHzLzpS221dm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SGD offers a lot of control over optimization procedure through a number of hypeparameters. However, we need to set them to right values in orderto make it work for training the model."
      ],
      "metadata": {
        "id": "8DWBUGfB27e_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.linear_model SGDRegressor\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import validation_curve\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "rvnejvCx3aC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(306)"
      ],
      "metadata": {
        "id": "qFmSqERW3rnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use ShuffleSplit as a cross validation strategy."
      ],
      "metadata": {
        "id": "DU44szHI3t1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle_split_cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "sSKmbVtH30cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the data and split it into train and test"
      ],
      "metadata": {
        "id": "4A_O3L6R37y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features, labels = fetch_california_housing(as_frame=True, return_X_y=True)\n",
        "com_train_features, test_features, com_train_labels, test_labels = train_test_split(features,labels,random_state=42)"
      ],
      "metadata": {
        "id": "TZL1aqAS4Aj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide the data into train and dev sets."
      ],
      "metadata": {
        "id": "9IvlPNxB4MQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features, dev_features, train_labels, dev_labels = train_test_split(com_train_features, com_train_labels, random_state=42)"
      ],
      "metadata": {
        "id": "M4MN6cSl4Pkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Baseline SGDRegressor\n",
        "\n",
        "1. To being with, we instantiate a baseline SGDRegressor model with default parameters.\n",
        "2. Train the model with training feature matrix and labels\n",
        "3. Obtain the score on the following and devel data."
      ],
      "metadata": {
        "id": "LlLNF-0o5BBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sgd = SGDRegressor(random_state=42)\n",
        "sgd.fit(train_features, train_labels)\n",
        "\n",
        "train_mae = mean_absolute_error(train_labels, sgd.predict(train_features))\n",
        "dev_mae = mean_absolute_error(dev_labels, sgd.predict(dev_features))\n",
        "print(train_mae, dev_mae)"
      ],
      "metadata": {
        "id": "ZQ906X_h6pSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training error is too high"
      ],
      "metadata": {
        "id": "7Tr-_KAi7GQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding a feature scaling step\n",
        "\n",
        "SGD is sensitive to feature scaling. Let's add a feature scaling steps and cehck if we get better MAE."
      ],
      "metadata": {
        "id": "O3ocQW717UtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sgd_pipeline = Pipeline([('feature_scaling', StandardScaler()),\n",
        "                         ('sgd',SGDRegressor())])\n",
        "sgd_pipeline.fit(train_features, train_labels)\n",
        "\n",
        "train_mae = mean_absolute_error(train_labels, sgd_pipeline.predict(train_features))\n",
        "dev_mae = mean_absolute_error(dev_labels, sgd_pipeline.predict(dev_features))\n",
        "print(train_mae, dev_mae)"
      ],
      "metadata": {
        "id": "P680ifHG9cjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error is still high, let's run SGDRegressor step by step and investigate issues with training."
      ],
      "metadata": {
        "id": "FkoMCUG19-gj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-wise training of SGDRegressor\n",
        "\n",
        "1. Instantiate SGDRegressor with warm_start=True and tol=-np.infty\n",
        "2. Train SGD step by step and record regression loss in each step\n",
        "3. Plot learning curves and see if there are any issues in training"
      ],
      "metadata": {
        "id": "u-dUGyLK-MsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eta0 = 1e-2\n",
        "sgd_pipeline = Pipeline([('feature_scaling', StandardScaler()),\n",
        "                         ('SGD',SGDRegressor(max_iter=1, tol=-np.infty,\n",
        "                                             warm_start=True,\n",
        "                                             random_state=42))])\n",
        "loss=[]\n",
        "for epoch in range(100):\n",
        "  sgd_pipeline.fit(train_features, train_labels)\n",
        "  loss.append(mean_squared_error(train_labels,\n",
        "                                 sgd_pipeline.predict(train_features)))\n",
        "\n",
        "plt.plot(np.arange(len(loss)), loss, 'b-')\n",
        "plt.xlabel('Iterator')\n",
        "plt.ylabel('MSE')\n",
        "plt.title(f'Learning curve: eta0={eta0:.4f}')"
      ],
      "metadata": {
        "id": "QfwaXSU4-lWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss reduced and increased, perhaps due to large learning rate, so we will try with a lower learning rate"
      ],
      "metadata": {
        "id": "ddnOMRk6BdZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eta0 = 1e-3\n",
        "sgd_pipeline = Pipeline([('feature_scaling', StandardScaler()),\n",
        "                         ('SGD',SGDRegressor(max_iter=1, tol=-np.infty,\n",
        "                                             warm_start=True,\n",
        "                                             random_state=42))])\n",
        "loss=[]\n",
        "for epoch in range(100):\n",
        "  sgd_pipeline.fit(train_features, train_labels)\n",
        "  loss.append(mean_squared_error(train_labels,\n",
        "                                 sgd_pipeline.predict(train_features)))\n",
        "\n",
        "plt.plot(np.arange(len(loss)), loss, 'b-')\n",
        "plt.xlabel('Iterator')\n",
        "plt.ylabel('MSE')\n",
        "plt.title(f'Learning curve: eta0={eta0:.4f}')"
      ],
      "metadata": {
        "id": "e_5rp9i-Bc2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An ideal learning curve"
      ],
      "metadata": {
        "id": "fXqZMNJxBsND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Iterations before reaching convergence\n",
        "print(sgd_pipeline[-1].n_iter_)\n",
        "\n",
        "#Weight updates\n",
        "print(sgd_pipeline[-1].t_)"
      ],
      "metadata": {
        "id": "TW6yh0xgBsAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mae = mean_absolute_error(train_labels, sgd_pipeline.predict(train_features))\n",
        "dev_mae = mean_absolute_error(dev_labels, sgd_pipeline.predict(dev_features))\n",
        "print(train_mae, dev_mae)"
      ],
      "metadata": {
        "id": "H5Iq4zyBCFxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fixing learning rate through validation curves\n",
        "\n",
        "1. Provide a list of hyper-parameters\n",
        "2. Instantiate an object of validation_curve with estimator, training features and label. Set scoring parameter to relevant score.\n",
        "3. Convert scores to error\n",
        "4. Plot validation curve with the value of hyper-parameter on x-axis and error on the y-axis\n",
        "5. Fix the hyper-parameter value where the test error is the least. "
      ],
      "metadata": {
        "id": "o92A2qr7Dbb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "eta0 = [1e-5,1e-4,1e-3,1e-2]\n",
        "train_scores, test_scores = validation_curve(sgd_pipeline, com_train_features, com_train_labels, param_name = 'SGD_eta0',\n",
        "                                             param_range=eta0, cv=shuffle_split_cv,scoring='neg_mean_squared_error',\n",
        "                                             n_jobs=2)\n",
        "train_errors, test_errors = -train_scores, -test_scores"
      ],
      "metadata": {
        "id": "pRFzqUQBFqJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_errors, test_errors = -train_scores, -train_scores\n",
        "plt.plot(eta0, train_scores.mean(axis=1),'b-x',labe='Training error')\n",
        "plt.plot(eta0, test_scores.mean(axis=1),'r-x',labe='Test error')\n",
        "plt.legend()\n",
        "plt.xlabel('eta0')\n",
        "plt.yabel('Mean absolute error')\n",
        "_=plt.title('Validation curve for SGD')"
      ],
      "metadata": {
        "id": "OKWRWLwoGYSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For eta=1e-3 the test error is the least and hence we select that value as the value for eta0\n",
        "Next we plot std dev in errors"
      ],
      "metadata": {
        "id": "Wdl0tTQYHofL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.errorbar(eta0, train_errrs.mean(axis=1), yerr=train_errors.std(axis=1), label='Training error')\n",
        "plt.errorbar(eta0, test_errrs.mean(axis=1), yerr=test_errors.std(axis=1), label='Test error')\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel('eta0')\n",
        "plt.ylabel('Mean absolute error')\n",
        "_=plt.title('Validation cruve for SGD')"
      ],
      "metadata": {
        "id": "N6maT0kBH1aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SGDRegressor"
      ],
      "metadata": {
        "id": "8dxhVJvVIObw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sgd_pipeline = Pipeline([('feature_scaling', StandardScaler()),\n",
        "                        ('sgd',SGDRegressor(max_iter=500,\n",
        "                                            early_stopping=True,\n",
        "                                            eta0=1e-3,\n",
        "                                            tol=1e-3,\n",
        "                                            validation_fraction=0.2,\n",
        "                                            n_iter_no_change=5,\n",
        "                                            average=10,\n",
        "                                            random_state=42))])\n",
        "sgd_pipeline.fit(train_features, train_labels)\n",
        "train_mae = mean_absolute_error(train_labels, sgd_pipeline.predict(train_features))\n",
        "dev_mae = mean_absolute_error(dev_labels, sgd_pipeline.predict(dev_features))\n",
        "print(train_mae,dev_mae)"
      ],
      "metadata": {
        "id": "pK2Nq-H7IXao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sgd_pipeline[-1].n_iter_,sgd_pipeline[-1].t_)"
      ],
      "metadata": {
        "id": "Cckcxi8oJTV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sgd_pipeline = Pipeline([('feature_scaling', StandardScaler()),\n",
        "                        ('sgd',SGDRegressor(max_iter=500,\n",
        "                                            early_stopping=True,\n",
        "                                            eta0=1e-3,\n",
        "                                            tol=1e-3,\n",
        "                                            learning_rate='adaptive',\n",
        "                                            validation_fraction=0.2,\n",
        "                                            n_iter_no_change=5,\n",
        "                                            average=10,\n",
        "                                            random_state=42))])\n",
        "sgd_pipeline.fit(train_features, train_labels)\n",
        "train_mae = mean_absolute_error(train_labels, sgd_pipeline.predict(train_features))\n",
        "dev_mae = mean_absolute_error(dev_labels, sgd_pipeline.predict(dev_features))\n",
        "print(train_mae,dev_mae)"
      ],
      "metadata": {
        "id": "IhsjkANwJst_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sgd_pipeline[-1].n_iter_,sgd_pipeline[-1].t_)"
      ],
      "metadata": {
        "id": "535VydOOJ2yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setting max_iters"
      ],
      "metadata": {
        "id": "kZjozD_iJ6cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_iter = np.ceil(1e6/com_train_features.shape[0])\n",
        "max-iter"
      ],
      "metadata": {
        "id": "5tkSphhZJ8VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sgd_pipeline = Pipeline([('feature_scaling', StandardScaler()),\n",
        "                        ('sgd',SGDRegressor(max_iter=max_iter,\n",
        "                                            early_stopping=True,\n",
        "                                            eta0=1e-3,\n",
        "                                            tol=1e-3,\n",
        "                                            learning_rate='constant',\n",
        "                                            validation_fraction=0.2,\n",
        "                                            n_iter_no_change=5,\n",
        "                                            average=10,\n",
        "                                            random_state=42))])\n",
        "sgd_pipeline.fit(train_features, train_labels)\n",
        "train_mae = mean_absolute_error(train_labels, sgd_pipeline.predict(train_features))\n",
        "dev_mae = mean_absolute_error(dev_labels, sgd_pipeline.predict(dev_features))\n",
        "print(train_mae,dev_mae)"
      ],
      "metadata": {
        "id": "Jvpa72Y7KuQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sgd_pipeline[-1].n_iter_,sgd_pipeline[-1].t_)"
      ],
      "metadata": {
        "id": "p2CRhmBLK8CC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
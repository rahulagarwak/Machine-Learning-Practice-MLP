{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_Wk5_Multiclass_Image_Classification_with_Perceptron.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multiclass Classifier (OneVsAll)\n",
        "\n",
        "* We know that the perceptron is a binary classifier. However, MNIST dataset contains 10 classes. Then how can we extend the idea to handle multi-class problems?\n",
        "* Solution: Combine multiple binary classifiers and devise a suitable scoring metric.\n",
        "* Sklearn makes it extremely easy without modifying a single line of code that we have written for the binary classifier.\n",
        "* Sklearn does this by counting a number of unique elements in the label vector y_train and converting labels using LabelBinarizer to fit each binary classifier"
      ],
      "metadata": {
        "id": "vNCir1r30_7_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SqZ6-a101tv"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.preprocessing import LabelBinarizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = Perceptron(random_state=1729)"
      ],
      "metadata": {
        "id": "Qv6U8E6u2t39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_ovr = LabelBinarizer().fit_transform(y_train)\n",
        "for i in range(10):\n",
        "  print('{0}:{1}'.format(y_train[i],y_train_ovr[i]))"
      ],
      "metadata": {
        "id": "hbFMlN9H2xZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The y_train_ovr will be of size of 60000 x 10\n",
        "* The first column will be a binary label vector for 0-detector and the next one for 1-Detector and so on."
      ],
      "metadata": {
        "id": "3W8zvUTq3RFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "tlwiuVil3qNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* What had actually happened internally was that the API automatically created 10 binary classifiers, converted labels to binary sparse matrix and trained them with the binarized labels!\n",
        "* During the inference time, the input will be passed through all these 10 classifiers and the highest score among the output from these will be considered as the predicted class.\n",
        "* To see it in action, let us execute the following lines of code."
      ],
      "metadata": {
        "id": "3u36IvC43tBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shape of Weight matrix: {0} and bias vector: {}'.format(clf.coef_.shape,clf.intercept_.shape))"
      ],
      "metadata": {
        "id": "OtznNmaQetOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* SO it is a matrix of size 10 x 784 where each row represents the weights for a single binary classifier.\n",
        "* Important difference to note is that there is no signum function associated with the perceptron.\n",
        "* The class of a perceptron that outputs the maximum score for the input sample is considered as the predicted class."
      ],
      "metadata": {
        "id": "NWUOjjQ6jt_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = clf.decision_function(x_train[6].reshape(-1,1))\n",
        "print(scores)\n",
        "print('The predicted class:'. np.argmax(scores))"
      ],
      "metadata": {
        "id": "EHZPmz42kgGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Predicted output:',clf.predict(x_train[0].reshape(-1,1)))"
      ],
      "metadata": {
        "id": "fELUQMezkx7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = clf.predict(x_train)"
      ],
      "metadata": {
        "id": "BWICBfT5k9Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_train, y_hat))"
      ],
      "metadata": {
        "id": "qX9te2m0lDl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us display the confusion matrix and relate it with the report above."
      ],
      "metadata": {
        "id": "xjd7eyeXlIOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm_display = ConfusionMatrixDisplay.from_prediction(y_train, y_hat, values_format='.5g')"
      ],
      "metadata": {
        "id": "n_pVJ1H5lPim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* What are all the insights we could infer from the above figure?\n",
        "* Digit 2 is often confused with Digit 3"
      ],
      "metadata": {
        "id": "RkwY8rKtmBYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making a Pipeline\n",
        "\n",
        "* Let's create a pipeline to keep the code compact.\n",
        "* Recall that, the MNIST dataset is clean and hence doesn't require much preprocessing\n",
        "* The one potential preprocessing technique we may use us to scale the features within the range (0,1)\n",
        "* It is not similar to scaling down the range values between 0 and 1."
      ],
      "metadata": {
        "id": "iyV2JQHnmRWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimators = [('std_scaler', MinMaxScaler()),('bin_clf',Perceptron())]\n",
        "pipe = Pipeline(estimators)"
      ],
      "metadata": {
        "id": "kBK0eWwjmvWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.fit(x_train,y_train_0)"
      ],
      "metadata": {
        "id": "ekalj1W8m9ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat_train_0 = pipe.predict(x_train)\n",
        "cm_display = ConfusionMatrixDisplay.from_prediction(y_train_0, y_hat_train_0,values_format='.5g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a_EH7fSbnDjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iteration vs Loss Curve\n",
        "\n",
        "The ther way of plotting iteration vs loss curve with the Partial_fit method"
      ],
      "metadata": {
        "id": "zj9W4CVAnV55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = 100\n",
        "bin_clf1 = Perceptron(max_iter=1000, random_state=2094)\n",
        "Loss_clf1 = []\n",
        "for i in range(iterations):\n",
        "  bin_clf1.partial_fit(x_train, y_train_0, classes=np.array([1,-1]))\n",
        "  y_hat_0 = bin_clf1.decision_function(x_train)\n",
        "  Loss_clf1.append(hinge_loss(y_train_0, y_hat_0))"
      ],
      "metadata": {
        "id": "rAG-YLoSniJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(np.arange(iterations), Loss_clf1)\n",
        "plt.grid(True)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O4mSFULXpHM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GridSearchCV\n",
        "\n",
        "* SO far we didn't do any hpt. We accepted the default value for learning rate of the perceptron class.\n",
        "* Now, lets search for a better learning rate using GridSearchCV.\n",
        "* No matter what the learning rate is, the loss will never converge to zero as the classes are not linearly separable."
      ],
      "metadata": {
        "id": "Gc52FjfIpaz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scoring = make_scorer(hinge_loss, greater_is_better=False)\n",
        "lr_grid = [1/2**n for n in range(1,6)]\n",
        "bin_clf_gscv = GridSearchCV(Perceptron(), param_grid={'eta0': lr_grid}, scoring=scoring, cv=5)\n",
        "bin_clf_gscv.fit(x_train, y_train_0)"
      ],
      "metadata": {
        "id": "i2RLCAD5p2T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(bin_clf_gscv.cv_results_)"
      ],
      "metadata": {
        "id": "lmNFhpvSqg4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see the best learning rate is 0.125"
      ],
      "metadata": {
        "id": "IPJ6kHm_qnr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = 100\n",
        "Loss = []\n",
        "best_bin_clf = Perceptron(max_iter=1000, random_state=2094, eta0=0.125)\n",
        "for i in range(iterations):\n",
        "  best_bin_clf = Perceptron(max_iter=1000, random_state=2094, eta0= 0.125)\n",
        "  y_hat_0 = best_bin_clf.decision_function(x_train)\n",
        "  Loss.append(hinge_loss(y_train_0,y_hat_0))\n"
      ],
      "metadata": {
        "id": "N14A7NidqrAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(np.arange(iterations), Loss_clf1, label='eta0=1')\n",
        "plt.plot(np.arange(iterations), Loss, label='eta0=0.125')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zgfuv0T-rQTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, instead of instatiating a Perceptron class with a new learning rate and re-train the model, we could simply get the best_estimator from GridSearchCV as follows."
      ],
      "metadata": {
        "id": "dIeP5BGVr8q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_bin_clf = bin_clf_gscv.best_estimator_"
      ],
      "metadata": {
        "id": "4WUQkTcfsMqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat_train_0 = bin_clf.predict(x_train)\n",
        "print(classification_report(y_teain_0, y_hat_train_0))"
      ],
      "metadata": {
        "id": "3JN6wJB5sTJM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
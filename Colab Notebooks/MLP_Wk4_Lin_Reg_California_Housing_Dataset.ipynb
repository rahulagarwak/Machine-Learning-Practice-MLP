{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_Wk4_Lin_Reg_California_Housing_Dataset.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regressions for house price prediction"
      ],
      "metadata": {
        "id": "NHzLzpS221dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy.stats import loguniform\n",
        "from scipy.stats import uniform\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.dummy import DummyRegressor\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_percentage_error\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import validation_curve\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n"
      ],
      "metadata": {
        "id": "RyTxekJnV5kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common set up"
      ],
      "metadata": {
        "id": "hO03ZVclVwO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set random seed\n",
        "np.random.seed(306)"
      ],
      "metadata": {
        "id": "PC6ouaI1W90c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure shuffle splt\n",
        "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "BL8spOKwXCuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and splitting"
      ],
      "metadata": {
        "id": "0s5MaK0oaDP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features, labels = fetch_california_housing(as_frame=True, return_X_y=True)\n",
        "com_train_features, test_features, com_train_labels, test_labels = train_test_split(features,labels,random_state=42)\n",
        "train_features, dev_features, train_labels, dev_labels = train_test_split(com_train_features, com_train_labels, random_state=42)"
      ],
      "metadata": {
        "id": "3lexvTqEaXXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression with normal equation"
      ],
      "metadata": {
        "id": "oErgIEWqar0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lin_reg_pipeline = Pipeline([('feature_scaling', StandardScaler()),\n",
        "                             ('lin_reg',LinearRegression())])\n",
        "lin_reg_cv_results = cross_validate(lin_reg_pipeline,\n",
        "                                    com_train_features,\n",
        "                                    com_train_labels,\n",
        "                                    cv=cv,\n",
        "                                    scoring='neg_mean_absolute_error',\n",
        "                                    return_train_score=True,\n",
        "                                    return_estimator=True)\n",
        "lin_reg_train_error = -1 * lin_reg_cv_results['train_score']\n",
        "lin_reg_test_error = -1 * lin_reg_cv_results['test_score']\n",
        "print(lin_reg_train_error.mean(),'+/-',lin_reg_train_error.std())\n",
        "print(lin_reg_test_error.mean(),'+/-',lin_reg_test_error.std())"
      ],
      "metadata": {
        "id": "noywbPbObY1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear regression with SGD\n",
        "\n",
        "Let's use iterative optimization method to train linear regression model.\n",
        "\n",
        "We use pipeline with two stages: 1. Feature Scaling and 2. SGD regression on the transformed feature matrix"
      ],
      "metadata": {
        "id": "hP8jNh8TlHWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sgd_reg_pipeline = Pipeline([('feature_scaling', StandardScaler()),\n",
        "                             ('sgd_reg',SGDRegressor(max_iter=np.ceil(1e6/com_train_features.shape[0]),\n",
        "                                            early_stopping=True,\n",
        "                                            eta0=1e-4,\n",
        "                                            learning_rate='constant',\n",
        "                                            tol=1e-5,\n",
        "                                            validation_fraction=0.1,\n",
        "                                            n_iter_no_change=5,\n",
        "                                            average=10,\n",
        "                                            random_state=42))])\n",
        "\n",
        "sgd_reg_cv_results = cross_validation(sgd_reg_pipeline,\n",
        "                              com_train_features,\n",
        "                              com_train_labels,\n",
        "                              cv=cv,\n",
        "                              scoring='neg_mean_absolute_error',\n",
        "                              return_train_score=True,\n",
        "                              return_estimator=True)\n",
        "\n",
        "sgd_train_error = -1 * sgd_reg_cv_results['train_score']\n",
        "sgd_test_error = -1 * sgd_reg_cv_results['test_score']\n",
        "print(sgd_train_error.mean(),'+/-',lin_reg_train_error.std())\n",
        "print(sgd_reg_test_error.mean(),'+/-',lin_reg_test_error.std())"
      ],
      "metadata": {
        "id": "Huel-bSmlXmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polynomial regression\n",
        "\n",
        "We will use degree 2 followed by validation curve"
      ],
      "metadata": {
        "id": "xOpKAHAimjdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_reg_pipeline = Pipeline([('poly', PolynomialFeatures(degree=2)),\n",
        "                              ('feature_scaling', StandardScaler()),\n",
        "                              ('lin_reg',LinearRegression())])\n",
        "poly_reg_cv_results = cross_validate(poly_reg_pipeline,\n",
        "                                    com_train_features,\n",
        "                                    com_train_labels,\n",
        "                                    cv=cv,\n",
        "                                    scoring='neg_mean_absolute_error',\n",
        "                                    return_train_score=True,\n",
        "                                    return_estimator=True)\n",
        "poly_reg_train_error = -1 * poly_reg_cv_results['train_score']\n",
        "poly_reg_test_error = -1 * poly_reg_cv_results['test_score']\n",
        "print(poly_reg_train_error.mean(),'+/-',poly_reg_train_error.std())\n",
        "print(poly_reg_test_error.mean(),'+/-',poly_reg_test_error.std())"
      ],
      "metadata": {
        "id": "_Ql-gIFcn5ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the reduction in error\n",
        "\n",
        "We would now use interaction feature terms only"
      ],
      "metadata": {
        "id": "uenoKvJUpgQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_reg_pipeline = Pipeline([('poly', PolynomialFeatures(degree=2, interaction_only=True)),\n",
        "                              ('feature_scaling', StandardScaler()),\n",
        "                              ('lin_reg',LinearRegression())])\n",
        "poly_reg_cv_results = cross_validate(poly_reg_pipeline,\n",
        "                                    com_train_features,\n",
        "                                    com_train_labels,\n",
        "                                    cv=cv,\n",
        "                                    scoring='neg_mean_absolute_error',\n",
        "                                    return_train_score=True,\n",
        "                                    return_estimator=True)\n",
        "poly_reg_train_error = -1 * poly_reg_cv_results['train_score']\n",
        "poly_reg_test_error = -1 * poly_reg_cv_results['test_score']\n",
        "print(poly_reg_train_error.mean(),'+/-',poly_reg_train_error.std())\n",
        "print(poly_reg_test_error.mean(),'+/-',poly_reg_test_error.std())"
      ],
      "metadata": {
        "id": "PDi_DGnCpm26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's figure out the best degree suited for regression task"
      ],
      "metadata": {
        "id": "B8LKvL4MpxX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "degree = [1,2,3,4,5]\n",
        "\n",
        "train_scores, test_scores = validation_curve(poly_reg_pipeline, com_train_features, com_train_labels, param_name='poly__degree',\n",
        "                                             param_range=degree, cv=cv, scoring='neg_mean_absolute_error', n_jobs=2)\n",
        "\n",
        "train_errors, test_errors = -train_scores, -test_scores\n",
        "plt.plot(degree, train_errors.mean(axis=1),'b-x',label='Training error')\n",
        "plt.plot(degree, test_errors.mean(axis=1),'b-x',label='Test error')\n",
        "plt.legend()\n",
        "\n",
        "plt.ylabel('degree')\n",
        "plt.xlabel('Mean absolute error (k$)')\n",
        "_=plt.title('Validation curve for polynomial regression')"
      ],
      "metadata": {
        "id": "BkecgS5Lp3Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ridge Regression"
      ],
      "metadata": {
        "id": "T1TjE9WiuCmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The polynomial models have a tendency to overfit- if we use higher order polynomial features. We will use Ridge regression-which penalizes for excessive model complexity in the polynomial regression by adding a regularization term. Here we specify the regularization rate alpha as 0.5 and train the regression model. Later we will launch hyperparameter search for the right value of alpha such that it leads to the least cross validation errors."
      ],
      "metadata": {
        "id": "fkGbZ7-kuGHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_reg_pipeline = Pipeline([('poly', PolynomialFeatures(degree=2)),\n",
        "                              ('feature_scaling', StandardScaler()),\n",
        "                              ('ridge',Ridge(alpha=0.5))])\n",
        "ridge_reg_cv_results = cross_validate(ridge_reg_pipeline,\n",
        "                                      com_train_features,\n",
        "                                      com_train_labels,\n",
        "                                      cv=cv,\n",
        "                                      scoring='neg_mean_absolute_error',\n",
        "                                      return_train_score=True,\n",
        "                                      return_estimator=True)\n",
        "ridge_reg_train_error = -1 * ridge_reg_cv_results['train_score']\n",
        "ridge_reg_test_error = -1 * ridge_reg_cv_results['test_score']\n",
        "print(ridge_reg_train_error.mean(),'+/-',ridge_reg_train_error.std())\n",
        "print(ridge_reg_test_error.mean(),'+/-',ridge_reg_test_error.std())"
      ],
      "metadata": {
        "id": "CLtuyIWlusOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HPT for ridge regularization rate"
      ],
      "metadata": {
        "id": "PQ7HFpEGvHbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_list = np.logspace(-4,0,num=20)\n",
        "ridge_reg_pipeline = Pipeline([('poly', PolynomialFeatures(degree=2)),\n",
        "                              ('feature_scaling', StandardScaler()),\n",
        "                              ('ridge_cv',RidgeCV(alpha=alpha_list,\n",
        "                                                  cv=cv,\n",
        "                                                  scoring='neg_mean_absolute_error'))])\n",
        "ridge_reg_cv_results = ridge_reg_pipeline.fit(com_train_features, com_train_labels)"
      ],
      "metadata": {
        "id": "lhJn18HAvLsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Score with best alpha', ridge_reg_cv_results[-1].best_score_)\n",
        "print('Error with best alpha', -ridge_reg_cv_results[-1].best_score_)\n",
        "print('Best alpha', ridge_reg_cv_results[-1].alpha_)"
      ],
      "metadata": {
        "id": "x5IAhLYwv1oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ridge HPT through GridSearchCV"
      ],
      "metadata": {
        "id": "kBxQ29QxxOfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_grid_pipeline = Pipeline([('poly', PolynomialFeatures(degree=2)),\n",
        "                                ('feature_scaling', StandardScaler()),\n",
        "                                ('ridge',Ridge())])\n",
        "\n",
        "param_grid = {'poly__degree':(1,2,3),\n",
        "              'ridge__alpha':np.logspace(-4,0,num=20)}\n",
        "\n",
        "ridge_grid_search = GridSearchCV(ridge_grid_pipeline,\n",
        "                                 param_grid=param_grid,\n",
        "                                 n_jobs=2,\n",
        "                                 cv=cv,\n",
        "                                 scoring='neg_mean_absolute_error',\n",
        "                                 return_train_score=True)\n",
        "ridge_grid_search.fit(com_train_features, com_train_labels)"
      ],
      "metadata": {
        "id": "KXKDUCq4xR1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ridge_ridge_search.best_index_ gives us the index of the best parameter in the list."
      ],
      "metadata": {
        "id": "h-T8b09Bycn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_train_error = -1 * ridge_grid_search.cv_results_['mean_train_score'][ridge_ridge_search.best_index_]\n",
        "mean_test_error = -1 * ridge_grid_search.cv_results_['mean_test_score'][ridge_ridge_search.best_index_]\n",
        "std_train_error = ridge_grid_search.cv_results_['std_train_score'][ridge_ridge_search.best_index_]\n",
        "std_test_error = ridge_grid_search.cv_results_['std_test_score'][ridge_ridge_search.best_index_]"
      ],
      "metadata": {
        "id": "clwoUARSycGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best mean absolute error of polynomial ridge regression model on the train set:', mean_train_error,' +/- ', std_train_error)\n",
        "print('Mean absolute error of polynomial ridge regression model on the test set:', mean_test_error,' +/- ', std_test_error)"
      ],
      "metadata": {
        "id": "ag6wVLTGzLZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Mean cross validated score', ridge_grid_search.best_score_)\n",
        "print('Mean cross validated error', -ridge_grid_search.best_score_)\n",
        "print('Best alpha', ridge_grid_search.best_params_)"
      ],
      "metadata": {
        "id": "JdPVJzQ-1AIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lasso regression"
      ],
      "metadata": {
        "id": "OwGUeH4I1luJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline model with fixed learning rate"
      ],
      "metadata": {
        "id": "2q8Qq4LI1oCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_reg_pipeline = Pipeline([('poly', PolynomialFeatures(degree=2)),\n",
        "                              ('feature_scaling', StandardScaler()),\n",
        "                              ('lasso',lasso(alpha=0.01))])\n",
        "lasso_reg_cv_results = cross_validate(lasso_reg_pipeline,\n",
        "                                      com_train_features,\n",
        "                                      com_train_labels,\n",
        "                                      cv=cv,\n",
        "                                      scoring='neg_mean_absolute_error',\n",
        "                                      return_train_score=True,\n",
        "                                      return_estimator=True)\n",
        "lasso_reg_train_error = -1 * lasso_reg_cv_results['train_score']\n",
        "lasso_reg_test_error = -1 * lasso_reg_cv_results['test_score']\n",
        "print(lasso_reg_train_error.mean(),'+/-',lasso_reg_train_error.std())\n",
        "print(lasso_reg_test_error.mean(),'+/-',lasso_reg_test_error.std())"
      ],
      "metadata": {
        "id": "d0DWS7eb1qjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HPT for lasso regularization rate"
      ],
      "metadata": {
        "id": "_GAGnJOM14Hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "with GridSearchCV"
      ],
      "metadata": {
        "id": "RPJZfRYc3L9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_grid_pipeline = Pipeline([('poly', PolynomialFeatures(degree=2)),\n",
        "                              ('feature_scaling', StandardScaler()),\n",
        "                              ('lasso',Lasso())])\n",
        "\n",
        "param_grid = {'poly__degree':(1,2,3),\n",
        "              'lasso_alpha': np.logspace(-4,0,num=20)}\n",
        "\n",
        "lasso_grid_search = GridSearchCV(lasso_grid_pipeline,\n",
        "                                 param_grid = param_grid,\n",
        "                                 n_jobs=2,\n",
        "                                 cv=cv,\n",
        "                                 scoring='neg_mean_absolute_error',\n",
        "                                 return_train_score=True)\n",
        "lasso_grid_search.fit(com_train_features, com_train_labels)"
      ],
      "metadata": {
        "id": "2hH6pYrc1-JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_train_error = -1 * lasso_grid_search.cv_results_['mean_train_score'][lasso_grid_search.best_index_]\n",
        "mean_test_error = -1 * lasso_grid_search.cv_results_['mean_test_score'][lasso_grid_search.best_index_]\n",
        "std_train_error = lasso_grid_search.cv_results_['std_train_score'][lasso_grid_search.best_index_]\n",
        "std_test_error = lasso_grid_search.cv_results_['std_test_score'][lasso_grid_search.best_index_]"
      ],
      "metadata": {
        "id": "BWRpGVRR4KA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best mean absolute error of polynomial lasso regression model on the train set:', mean_train_error,' +/- ', std_train_error)\n",
        "print('Mean absolute error of polynomial lasso regression model on the test set:', mean_test_error,' +/- ', std_test_error)"
      ],
      "metadata": {
        "id": "rLyARFU74tj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Mean cross validated score', lasso_grid_search.best_score_)\n",
        "print('Mean cross validated error', -lasso_grid_search.best_score_)\n",
        "print('Best alpha', lasso_grid_search.best_params_)"
      ],
      "metadata": {
        "id": "-IuePSPy4tj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SGD: Regularization and HPT"
      ],
      "metadata": {
        "id": "OABrCVR95ffQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_sgd_pipeline = Pipeline([('poly', PolynomialFeatures(degree=2)),\n",
        "                              ('feature_scaling', StandardScaler()),\n",
        "                              ('sgd_reg',SGDRegressor(penalty='elasticnet',\n",
        "                                                      random_state=42))])\n",
        "poly_sgd_cv_results = cross_validate(poly_sgd_pipeline,\n",
        "                                      com_train_features,\n",
        "                                      com_train_labels,\n",
        "                                      cv=cv,\n",
        "                                      scoring='neg_mean_absolute_error',\n",
        "                                      return_train_score=True,\n",
        "                                      return_estimator=True)\n",
        "poly_sgd_train_error = -1 * poly_sgd_cv_results['train_score']\n",
        "poly_sgd_test_error = -1 * poly_sgd_cv_results['test_score']\n",
        "print(poly_sgd_train_error.mean(),'+/-',poly_sgd_train_error.std())\n",
        "print(poly_sgd_test_error.mean(),'+/-',poly_sgd_test_error.std())"
      ],
      "metadata": {
        "id": "_JUGNdw052pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's search for best set of params for ploy+SGD"
      ],
      "metadata": {
        "id": "D5-KD4kQAZiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class uniform_int:\n",
        "  def __init__(self,a,b):\n",
        "    self.distribution = uniform(a,b)\n",
        "  def rvs(self, *args, **kwargs):\n",
        "    return self._distribution.rvs(*args, **kwargs).astype(int)"
      ],
      "metadata": {
        "id": "eu--saJAAgPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's specify RandomizedSearchCV setup"
      ],
      "metadata": {
        "id": "mNfq5_7vA9T1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_distributions = {\n",
        "    'poly__degree':[1,2,3],\n",
        "    'sgd_reg__learning_rate':['constant','adaptive','invscalig'],\n",
        "    'sgd_reg__l1_ratio':uniform(0,1),\n",
        "    'sgd_reg__eta0':loguniform(1e-5,1),\n",
        "    'sgd_reg__power_t':uniform(0,1)\n",
        "}\n",
        "\n",
        "poly_sgd_random_search = RandomizedSearchCV(\n",
        "    poly_sgd_pipeline, param_dsitributions=param_distributions,\n",
        "    n_iter=10,cv=cv,verbose=1,scoring='neg_mean_absolute_error'\n",
        ")\n",
        "\n",
        "poly_sgd_random_search.fit(com_train_features, com_train_labels)"
      ],
      "metadata": {
        "id": "7psW6alZBHSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best score can be obtained as follows:"
      ],
      "metadata": {
        "id": "q4wII0hfCBTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_sgd_random_search.best_score_"
      ],
      "metadata": {
        "id": "KC3Jo-19CD-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best set of parameters are obtained as follows:"
      ],
      "metadata": {
        "id": "rvIYzfNqCGXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_sgd_random_search.best_params_"
      ],
      "metadata": {
        "id": "4-xifUJ5CJth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and the best estimator can be accessed with best_estimator_ member variable."
      ],
      "metadata": {
        "id": "lF0x2gr7Cix8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison of weight vectors\n",
        "\n",
        "Let's look at the weight vectors produced by different models."
      ],
      "metadata": {
        "id": "WuopgRJECs8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = poly_reg_cv_results['estimator'][0][0].get_feature_names_out(\n",
        "    input_features=train_features.columns)\n",
        "feature_names"
      ],
      "metadata": {
        "id": "Tc4BgJAKC1Xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coefs = [est[-1].coef_ for est in poly_reg_cv_results[estimator]]\n",
        "weights_polynomial_regression = pd.DataFrame(coefs, columns=feature_names)"
      ],
      "metadata": {
        "id": "FbZr5XcHC1m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "color = {'whiskers':'black','medians':'black','caps':'black'}\n",
        "weights_polynomial_regression.plot.box(color=color,vert=False,figsize=(6,16))\n",
        "_=plt.title('Polynomial regression coefficients')"
      ],
      "metadata": {
        "id": "Fr16TJC6Dwi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance on the test set\n",
        "\n",
        "Baseline"
      ],
      "metadata": {
        "id": "J6iag3-hEUdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model_median = DummyRegressor(strategy='median')\n",
        "baseline_model_median.fit(train_features, train_labels)\n",
        "mean_absolute_percentage_error(test_labels,baseline_model_median.predict(test_features))"
      ],
      "metadata": {
        "id": "ooOr3A6wEXs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear regression with normal equation"
      ],
      "metadata": {
        "id": "nMBO-EciEr4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_absolute_percentage_error(test_labels,lin_reg_cv_results['estimator'][0][0].predict(test_features))"
      ],
      "metadata": {
        "id": "bfdTOi6JEvaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression"
      ],
      "metadata": {
        "id": "dvzrzJH_FUJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_absolute_percentage_error(test_labels,poly_sgd_random_search.best_estimator_.predict(test_features))"
      ],
      "metadata": {
        "id": "6GogvjPrFD8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge Regression"
      ],
      "metadata": {
        "id": "RHuWWE85FWlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_absolute_percentage_error(test_labels,ridge_grid_search.best_estimator_.predict(test_features))"
      ],
      "metadata": {
        "id": "P7wjUFSFGyqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso Regression"
      ],
      "metadata": {
        "id": "jMS0dajoGxdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_absolute_percentage_error(test_labels,lasso_grid_search.best_estimator_.predict(test_features))"
      ],
      "metadata": {
        "id": "zW4iXFaiG5nw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
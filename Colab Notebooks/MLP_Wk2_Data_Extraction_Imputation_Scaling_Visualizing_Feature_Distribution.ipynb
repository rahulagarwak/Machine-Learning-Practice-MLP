{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLP_Wk2_Data_Extraction_Imputation_Scaling_Visualizing_Feature_Distribution.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMCAf19fciPaxGYfmMBAQ7m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data Pre-processing Techniques"],"metadata":{"id":"uSuWyto7fybK"}},{"cell_type":"markdown","source":["Data preprocessing involves several transformations that are applied to the raw data to make it more amenable for learning. It is carried out before using it for model training or prediction.\n","\n","There are many pre-processing techniques for\n","\n","* Data Cleaning\n","  * Data Imputation\n","  * Feature scaling\n","\n","* Feature Transformation\n","  * Polynomial Features\n","  * Discretization\n","  * Handling categorical features\n","  * Custom Transformers\n","  * Composite Transformers\n","    * Apply transformation to diverse features\n","    * TargetTransformedRegresor\n","* Feature Selection\n","  * Filter based feature selection\n","  * Wrapper based feature selection\n","* Feature Extraction\n","  * PCA"],"metadata":{"id":"HEBYfOW-f2ly"}},{"cell_type":"markdown","source":["The transformations are applied in a specific order and the order can be specified via Pipeline. We need to apply different transformations based on the feature type. FeatureUnion helps us perform that task and combine outputs from multiple transformations into a single transformed feature matrix. We will also study as how to visualize this pipeline."],"metadata":{"id":"o0nIlECXp485"}},{"cell_type":"markdown","source":["# Importing basic Libraries"],"metadata":{"id":"TMpg2oggqaMh"}},{"cell_type":"markdown","source":["In this colab, we are importing libraries as needed. However it is a good practice to have all imports in one cell-arranged in an alphabetical order. This helps us weed out any duplicate imports and some such issues."],"metadata":{"id":"Fj2jtFJjqddP"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sns\n","sns.set_theme(style='whitegird')"],"metadata":{"id":"dui99KSnqvLm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. Feature Extraction"],"metadata":{"id":"jU5aNxFyq6HJ"}},{"cell_type":"markdown","source":["## DictVectorizer"],"metadata":{"id":"t8pXg5KRq9z-"}},{"cell_type":"markdown","source":["Many a times the data is present as a list of dictionary objects. ML algorithms expect the data in matrix form with shape (n,m) where n is the number of samples and m is the number of features.\n","\n","DictVectorizer converts a list of dictionary objects to feature matrix.\n","\n","Let's create a sample data for demo purpose containing age and height of children.\n","\n","  Each record/sample is a dictionary with two keys age and height and corresponding values."],"metadata":{"id":"D8Vv8TnLrAMx"}},{"cell_type":"code","source":["data = [{'age':4,'height':96.0},\n","        {'age':1,'height':73.9},\n","        {'age':3,'height':88.9},\n","        {'age':2,'height':81.6}]"],"metadata":{"id":"Iz-5G7qyr5lV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There are 4 data samples with 2 features each."],"metadata":{"id":"lqt5QQiQsN-E"}},{"cell_type":"markdown","source":["Let's make use of DictVectorizer to convert the list of dictionary objects to the feature matrix."],"metadata":{"id":"93ISTU--slT3"}},{"cell_type":"code","source":["from sklearn.feature_extraction import DictVectorizer\n","dv = DictVectorizer(sparse=False)\n","data_transformed = dv.fit_transform(data)\n","data_transformed"],"metadata":{"id":"mcXwmAn3sspR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_transformed.shape"],"metadata":{"id":"7JW_UNqts6rA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The transformed data is in a feature matrix form-4 samples with 2 features each i.e. shape (4,2)"],"metadata":{"id":"zm7ebv-ftAiM"}},{"cell_type":"markdown","source":["# 2. Data Imputation"],"metadata":{"id":"oWuPqqYDtNTy"}},{"cell_type":"markdown","source":["* Many machine learning algorithms need full feature matrix and they may not work in presence of missin data.\n","* Data imputation identifies missing values in each feature of the dataset and replaces them with an appropriate value based on a fixed strategy such as\n","  * mean or median or mode of that feature.\n","  * use specified constant value.\n","\n","Sklearn library provides sklearn.impute.SimpleImputer class for this purpose."],"metadata":{"id":"zw3loop0tPnQ"}},{"cell_type":"code","source":["from sklearn.impute import SimpleImputer"],"metadata":{"id":"VtzYhnPduOBM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Some of its important parameters:\n","\n","* missing_values: Could be int, float, str, np.nan or None. Default is np.nan\n","* strategy: string, default is 'mean'. One of following strategies can be used:\n","\n","  * mean- missing values are replaced using the mean along each column.\n","  * median- missing values are replaced using the median along each column.\n","  * most_frequent- missing values are replaced using the most frequent along each column.\n","  * constant- missing values are replaced using the fill_value arguement.\n","* add_indicator is a boolean parameter that when set to True returns missing value indicators in indicator_ member variable.\n","\n","Note:\n","* mean and mode strategies can only be used with numeric data.\n","* most_frequent and constant strategies can be used with strings or numeric data."],"metadata":{"id":"lhVeKwTFuSp4"}},{"cell_type":"markdown","source":["# Data imputation on real world dataset"],"metadata":{"id":"FxUjbXbsvfc9"}},{"cell_type":"markdown","source":["Let's perform data imputation on real world dataset. We will be using heart disease dataset from uci machine learning repo for this purpose. We will load this dataset from csv file."],"metadata":{"id":"5wY3mj0DvlMP"}},{"cell_type":"code","source":["cols = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','num']\n","heart_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data',header=None, names=cols)"],"metadata":{"id":"1onC31u9v4A7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The dataset has the following features:"],"metadata":{"id":"Esv68hGRyvVo"}},{"cell_type":"markdown","source":["1. Age (in years)\n","2. Sex (1=male; 0=female)\n","3. cp - cheap pain type\n","4. trestbps - resting blood pressure (anything above 130-140 is typically cause for concern)\n","5. chol - serum cholestrol in mg/dl (above 200 is cause for concern)\n","6. fbs -  fasting blood sugar (>120 mg/dl)(1=true;0=false)\n","7. restecg - resting electrocardiographic results\n","  * 0=normal\n","  * 1=having ST-T wave abnormality\n","  * 2=showing probable or definite left ventricular hypertrophy by Estes' criteria\n","8. thalach - maximum heart rate achieved\n","9. exang - exercise induced angina\n","  * 1=yes\n","  * 0=no\n","10. oldpeak - depression induced by exercise relative to rest\n","11. slope - slope of peak exercise ST segment\n","  * 1=upsloping\n","  * 2=flat value\n","  * 3=downsloping\n","12. ca = number of major vessels (0-3) colored by flourosopy\n","13. thal - (3=normal; 6=fixed defect; 7=reversable defect)\n","14. num - diagnosis of heart disease (angiographic disease status)\n","  *0: <50% diameter narrowing\n","  *1: >50% diameter narrowing"],"metadata":{"id":"LaoU7Ldgyyw6"}},{"cell_type":"markdown","source":["**STEP 1:** Check if the dataset contains missing values.\n","\n","* This can be checked via dataset description or by check number of nan or np.null in the dataframe. However such a check can be performed only for numerical features.\n","* Fr non-numerical features, we can list their unique values and check if there are values like $?$."],"metadata":{"id":"pItXxnlX4B3Y"}},{"cell_type":"code","source":["heart_data.info()"],"metadata":{"id":"kDMCgTM24e-A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's check if there are any missing values in numerical columns-here we have checked it for all columns in the dataframe."],"metadata":{"id":"PsKXo-zl6ekb"}},{"cell_type":"code","source":["(heart_data.isnull().sum())"],"metadata":{"id":"9-KDh2Rw6dGj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There are two non-numerical features: ca and thal.\n","\n","* List their unique values"],"metadata":{"id":"aWKaWgK76yyB"}},{"cell_type":"code","source":["print(heart_data.ca.unique(),heart_data.thal.unique())"],"metadata":{"id":"XMEkx4a767C1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Both of them contain ?, which is a missing values. Let's count the number of missing values."],"metadata":{"id":"UHkaK0-F7FdU"}},{"cell_type":"code","source":["print(heart_data.loc[heart_data.ca=='?','ca'].count(),heart_data.loc[heart_data.thal==?,'thal'].count())"],"metadata":{"id":"8pPamzDa7NKh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 2:** Replace '?' with nan"],"metadata":{"id":"cV1TrN517wGL"}},{"cell_type":"code","source":["heart_data.replace('?',np.nan,inplace=True)"],"metadata":{"id":"cla6LPFz70u-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 3:** Fill the missing values with sklearn missing value imputation utilities\n","\n","Here we use SimpleImputer with mean strategy.\n","\n","We will try two variations-\n","\n","* add_indicator = False: Default choice that only imputes missing values."],"metadata":{"id":"Xf61ZdEa76Du"}},{"cell_type":"code","source":["imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n","imputer = imputer.fit(heart_data)\n","heart_data_imputed = imputer.transform(heart_data)\n","print(heart_data_imputed.shape)"],"metadata":{"id":"WnwlaaeD8Wcg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* add_indicator = True: Adds additonal column for each column containing missing values, In our case, this adds two columns one for ca and other for thal. It indicates if the sample has missing value."],"metadata":{"id":"658LCi588wKI"}},{"cell_type":"code","source":["imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean',add_indicator = True)\n","imputer = imputer.fit(heart_data)\n","heart_data_imputed = imputer.transform(heart_data)\n","print(heart_data_imputed.shape)"],"metadata":{"id":"4rd-g5cy-LKx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Feature scaling"],"metadata":{"id":"w6bDZ1bW-Rhb"}},{"cell_type":"markdown","source":["Feature scaling transforms feature values such that all the features are on the same scale.\n","\n","When we use feature matrix with all features on the same scale, it provides us certain advantages as listed below:\n","\n","* Enables faster convergence in iterative optimization algorithms like gradient descent and its variants.\n","* The performance of ML algorithms such as SVM, K-NN and K-menas etc that compute euclidean distance among input samples gets impacted if the features are not scaled.\n","\n","Tree based ML algorithms are not affected by feature-scaling. In other words, feature scaling is not required for tree based ML algorithms.\n","\n","feature scaling can be performed with the following methods:\n","\n","* Standardization\n","* Normalization\n","* MaxAbsScaler\n","\n","Let's demonstrate feature scaling on a real world dataset. For this purpose we will be using ablone dataset. We will use different scaling utilities in sklearn library."],"metadata":{"id":"Pc1JHBnB-WQf"}},{"cell_type":"code","source":["cols = ['Sex','Length','Diameter','Height','Whole weight','Shucked weight','Viscera weight','Shell weight','Rings']\n","abalone_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data',header=None, names=cols)"],"metadata":{"id":"O4_S7eiGAywi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 1:** Examine the dataset\n","\n","Feature scaling is performed only on numerical attributes. Let's check which are numerical attributes in this dataset. We can get that via info() method"],"metadata":{"id":"fvCn-ve-Byzm"}},{"cell_type":"code","source":["abalone_data.info()"],"metadata":{"id":"19Wk6_7mCT-d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 1a:** [Optional]: Convert non-numerical attributes to numerical ones.\n","\n","In this dataset, Sex is a non-numeric column in this dataset. Let's examine it and see if we can convert it to numeric representation."],"metadata":{"id":"c4csjfd5Cvtz"}},{"cell_type":"code","source":["abalone_data.Sex.unique()"],"metadata":{"id":"GAmiCejdNslM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["abalone_data = abalone_data.replace({'Sex':{'M':1,'F':2,'I':3}})\n","abalone_data.info()"],"metadata":{"id":"Lkx6-SZ5Nw2R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 2:** Separate labels from features"],"metadata":{"id":"La_nI8ObOBCh"}},{"cell_type":"code","source":["y = abalone_data.pop('Rings')\n","abalone_data.info()"],"metadata":{"id":"2cKCDKbgOF1s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 3:** Examine feature scales"],"metadata":{"id":"0hUUo_coOZxe"}},{"cell_type":"markdown","source":["Statistical method\n","\n","Check the scales of different feature with describe() method of dataframe."],"metadata":{"id":"IT1cirTqOdx7"}},{"cell_type":"code","source":["abalone_data.describe().T"],"metadata":{"id":"WmvQZxH9Oj4a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that\n","\n","* There are 4177 examples or rows in this dataset.\n","* The mean and standard deviation of features are quite different from one another.\n","\n","We can confirm that with a variety of visualization techniques and plots."],"metadata":{"id":"HVebSRNJOndl"}},{"cell_type":"markdown","source":["## Visualization of feature distributions"],"metadata":{"id":"tPQKeySfO_V0"}},{"cell_type":"markdown","source":["Visualize feature distributions\n","\n","* Histogram\n","* Kernel density estimation KDE plot\n","* Box\n","* Violin"],"metadata":{"id":"rxsJ-nJzPHKO"}},{"cell_type":"markdown","source":["Feature Histogram\n","\n","We will have separate and combined histogram plots to check if the feature are indeed on different scales."],"metadata":{"id":"mu8Zgh-ZP0LR"}},{"cell_type":"markdown","source":["# to be added"],"metadata":{"id":"8uBA0DNJQ1eJ"}},{"cell_type":"markdown","source":["**Step 4:** Scaling"],"metadata":{"id":"xCj2mGMnQ03S"}},{"cell_type":"markdown","source":["* Normalization\n","* MaxAbsScaler\n","* MinMaxScaler"],"metadata":{"id":"peAiC0RKWhpc"}},{"cell_type":"code","source":[""],"metadata":{"id":"L3bpMVQsW5K9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import MaxAbsScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler\n","\n","x = np.array([4,2,5,-2,-100]).reshape(-1,1)\n","mas = MaxAbsScaler()\n","x_mas = mas.fit(x)\n","\n","x = abalone_data\n","mm = MinMaxScaler()\n","x_n = mm.fit_transform(x)\n","\n","ss = StandardScaler()\n","x_s = ss.fit_transform(x)\n","\n","print(x_mas)\n","print(x_n)\n","print(x_s)"],"metadata":{"id":"gkWJAz88WsHe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. add_dummy_feature\n","\n","Augments dataset with a column vector, each value in the column vector is 1. This is useful for adding a parameter for bias term in the model."],"metadata":{"id":"cD6ipUrlcqLr"}},{"cell_type":"code","source":["x = np.array([[7,1],[1,8],[2,0],[9,6]])\n","\n","from sklearn.preprocessing import add_dummy_feature\n","\n","x_new = add_dummy_feature(x)\n","x_new"],"metadata":{"id":"ul-y3VsPfFij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Custom transformers"],"metadata":{"id":"oDwIUpu6fdva"}},{"cell_type":"markdown","source":["Enables conversion of an existing Python function into a tranformer to assist in data cleaning or processing.\n","\n","Useful when:\n","\n","1. The dataset consists of heterogeneous data type\n","2. The dataset is stored in a pandas dataframe and different columns require different processing pipelines\n","3. We need stateless transformations such as taking the log of frequencies , custom scaling etc"],"metadata":{"id":"sqadUndNfioI"}},{"cell_type":"code","source":["from sklearn.preprocessing import FunctionTransformer"],"metadata":{"id":"2hmC7SBkg6oo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can implement a transformer from an arbitrary function with FunctionTransformer"],"metadata":{"id":"zvihyNsSg-lZ"}},{"cell_type":"code","source":["wine_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv',sep=';')"],"metadata":{"id":"1hjmNSNEhKzQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wine_data.describe().T"],"metadata":{"id":"hXWzhX-XhZPV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's use np.log1p which returns natural logarithm of 1+feature value"],"metadata":{"id":"HjKwI9Nvhc4j"}},{"cell_type":"code","source":["transformer = FunctionTransformer(np.log1p, validate=True)\n","wine_data_trasformed = transformer.transform(np.array(wine_data))\n","pd.DataFrame(wine_data_trasformed, columns=wine_data.columns).describe().T"],"metadata":{"id":"NU4LjhXJhkQv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6. Polynomial Features"],"metadata":{"id":"N3FHYFcLh7dv"}},{"cell_type":"markdown","source":["Generate a new feature consisting of all polynomial combinations of the features with degree less than or equal to the specified degree.\n","\n","* For example, if an input sample is two dimensional and of the form [a,b], the degree-2 polynomial features are [1,a,b,a^2 ,ab,b^2]\n","\n"],"metadata":{"id":"xmyeEBdiUJ_J"}},{"cell_type":"code","source":["from sklearn.preprocessing import PolynomialFeatures\n","\n","wine_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv',sep=';')\n","wine_data_copy = wine_data.copy()\n","wine_data = wine_data.drop(['quality'],axis=1)\n","print(wine_data.shape)\n","\n","poly = PolynomialFeatures(degree=2)\n","poly_wine_data = poly.fit_transform(wine_data)\n","print(poly_wine_data.shape)"],"metadata":{"id":"dvlu0yBijRDu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After transformation we have 78 features, let's list them out"],"metadata":{"id":"TKym5rL9j4iz"}},{"cell_type":"code","source":["poly.get_feature_names_out()"],"metadata":{"id":"LlYpmOEZj8d4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#7. Discretization\n","\n","Discretization/quantization/binning provides a way to partition continuous features into discrete values."],"metadata":{"id":"X1o_JxQukDpQ"}},{"cell_type":"code","source":["from sklearn.preprocessing import KBinsDiscretizer"],"metadata":{"id":"Zs-EOrn7kZEE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let us demonstrate KBinsDiscretizer using wine quality dataset."],"metadata":{"id":"J5WTyPZAkdpV"}},{"cell_type":"code","source":["wine_data = wine_data_copy.copy()\n","\n","enc = KBinsDiscretizer(n_bins=10, encode='onehot')\n","X=np.array(wine_data['chloride']).reshape(-1,1)\n","X_binned = enc.fit_transform(X)\n","X_binned"],"metadata":{"id":"A8PdveH1kjIm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_binned.toarray()[:5]"],"metadata":{"id":"Qd6PnUG8qeuu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 8. Handling Categorical Features\n","\n","We need to convert the categorical features into numerical features\n","\n","1. Ordinal encoding\n","2. One-Hot encoding\n","3. Label encoder\n","4. Using dummy variables"],"metadata":{"id":"_sxKpfZ0qpN3"}},{"cell_type":"code","source":["from sklearn.preprocessing import OrdinalEncoder\n","from sklearn.preprocessing import OneHotEncoder\n","\n","cols = ['sepal length','sepal width','petal length','petal width','label']\n","iris_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',header=None, names=cols)\n","\n","onehotencoder = OneHotEncoder(categories='auto')\n","iris_labels = onehotencoder.fit_transform(iris_data.label.values.reshape(-1,1))\n","iris_labels.toarray()[:5]"],"metadata":{"id":"l3YOKOmrrVBP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let us observe the difference between one hot encoding and ordinal encoding."],"metadata":{"id":"pEC0CjwKsxiN"}},{"cell_type":"code","source":["enc = OrdinalEncoder()\n","iris_labels = np.array(iris_data['label'])\n","\n","iris_labels_transformed = enc.fit_transform(iris_labels.reshape(-1,1))\n","print(np.unique(iris_labels_transformed))\n","print(iris_labels_transformed[:5])"],"metadata":{"id":"oaRSvbSqs2ij"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","iris_labels = np.array(iris_data['label'])\n","enc = LabelEncoder()\n","label_integer = enc.fit_transform(iris_labels)\n","label_integer"],"metadata":{"id":"Z-onTm8Jt6QI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["movie_genres = [{'action','comedy'},{'comedy'},{'action','thriller'},{'science-fiction','action','thriller'}]\n","from sklearn.preprocessing import MultiLabelBinarizer\n","mlb = MultiLabelBinarizer()\n","mlb.fit_transform(movie_genres)"],"metadata":{"id":"Rxypn-pQuMpZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Using dummy variables"],"metadata":{"id":"YEo5I_o-utFO"}},{"cell_type":"code","source":["iris_data_onehot = pd.get_dummies(iris_data, columns=['label'],prefix=['one_hot'])\n","iris_data_onehot"],"metadata":{"id":"q8MF3rl6uwC_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 9. Composite Transformers"],"metadata":{"id":"chzKerM6uvq0"}},{"cell_type":"markdown","source":["It applies a set of transformers to columns of an array or pandas.DataFrame, concatanates the transformed outputs from different transformers into a single matrix.\n","\n","* It is useful for transforming heterogenous data by applying different transformers to separate subsets of features.\n","* It combines different feature selection mechanisms and transformation into a single transformer object."],"metadata":{"id":"Xyl93C1mvF-F"}},{"cell_type":"code","source":["x = [[20.0,'male'],[11.2,'female'],[15.6,'female'],[13.0,'male'],[18.6,'male'],[16.4,'female']]\n","x = np.array(x)"],"metadata":{"id":"-61IhDJAvqL9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.compose import ColumnTransformer\n","from sklearn.compose import MaxAbsScaler, OneHotEncoder\n","\n","ct = ColumnTransformer([('scaler',MaxAbsScaler(),[0]),\n","                        ('pass','passthrough',[0]),\n","                        ('encoder',OneHotEncoder(),[1])])\n","ct.fit_transform(x)"],"metadata":{"id":"CHKGMzKXwDDt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TransformedtargetRegressor"],"metadata":{"id":"aoXWMmEgw1LB"}},{"cell_type":"code","source":["from sklearn.compose import TransformedTargetRegressor\n","from sklearn.preprocessing import MaxAbsScaler\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import fetch_california_housing\n","\n","X,y = fetch_california_housing(return_X_y=True)\n","X,y = X[:2000,:], y[:2000]\n","\n","transformer = MaxAbsScaler()\n","\n","regressor = LinearRegression()\n","\n","regr = TransformedTargetRegressor(regressor=regressor,transformer=transformer)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0)\n","regr.fit(X_train, y_train)\n","print(regr.score(X_test, y_test))\n","raw_target_regr = LinearRegression.fit(X_train,y_train)\n","print(raw_target_regr.score(X_test,y_test))"],"metadata":{"id":"pgzDgpFExEi7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 10. Feature Selection\n","\n","sklearn.feature_selection module has useful APIs to select features/reduce dimensionality, either to improve estimators accuracy scores or to boost their performance on very high-dimensional datasets."],"metadata":{"id":"7JyyE2QGUE-h"}},{"cell_type":"markdown","source":["##Filter based methods"],"metadata":{"id":"yaRr7LjbUgvN"}},{"cell_type":"markdown","source":["VarianceThreshold\n","\n","This transformer helps to keep only high variance features by providing a certain threshold.\n","\n","Features with variance greater or equal to threhold value are kept rest are removed.\n","\n","By default it removes any feature with same value ie 0 variance"],"metadata":{"id":"L4FQwXX1Uk3n"}},{"cell_type":"code","source":["data = [{'age':4,'height':96.0},\n","        {'age':1,'height':73.9},\n","        {'age':3,'height':88.9},\n","        {'age':2,'height':81.6}]\n","\n","dv = DictVectorizer(sparse=False)\n","data_transformed = dv.fit_transform(data)\n","np.var(data_transformed, axis=0)"],"metadata":{"id":"zOUvPlylU_k5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_selection import VarianceThreshold\n","vt = Threshold(threshold=9)\n","data_new = vt.fit_transform(data_transformed)\n","data_new"],"metadata":{"id":"i_vWBGlIVTx8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you may observe from output of above cell, the transformer has removed the age feature because its variance is below he threshold."],"metadata":{"id":"KyUuUEFzVkjp"}},{"cell_type":"markdown","source":["SelectKBest\n","\n","It selects k highest scoring features based on a function and removes the rest of the features.\n","Lets take an example of California Housing dataset."],"metadata":{"id":"F3ZcCajqVwmI"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_selection import SelectKBest, mutual_info_regression\n","\n","X_cal, y_cal = fetch_california_housing(return_X_y=True)\n","\n","X,y = X_cal[:2000,:], y_cal[:2000]\n","\n","X.shape"],"metadata":{"id":"8ce4PiVtWBYU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's take 3 most important features, since it is a regression problem, we can use mutual_info_regression or f_regression scoring functions only."],"metadata":{"id":"1C8Sr8QzWf-e"}},{"cell_type":"code","source":["skb = SelectKBest(mutual_info_regression, k=3)\n","X_new = skb.fit_transform(X,y)\n","X_new.shape"],"metadata":{"id":"4hdGoBOLWrEh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["skb.get_features_names_out()"],"metadata":{"id":"FcXhAHeIXaEy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["SelectPercentile\n","\n","This is very similar to SelectKBest from previous section"],"metadata":{"id":"FD36TuMyW48b"}},{"cell_type":"code","source":["from sklearn.feature_selection import SelectPercentile\n","sp = SelectPercentile(mutual_info_regression, percentile=30)\n","X_new = s.fit_transform(X,y)\n","X_new.shape"],"metadata":{"id":"sKX2hI2BXFUR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sp.get_features_names_out()"],"metadata":{"id":"8ZQ_tYj9XUtZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GenericUnivariateSelect\n","\n","It applies univariate feature selection with a certain strategy, which is passed to the API via mode parameter, mode can take one of the following values: percentile,k_best,fpr,fdr,fwe\n","\n","for similar to SelectKBest results, below is the code"],"metadata":{"id":"A1WJH8OaXdxf"}},{"cell_type":"code","source":["from sklearn.feature_selection import GenericUnivariateSelect\n","\n","gud = GenericUnivariateSelect(mutual_info_regression, mode = k_best, param=3)\n","X_new = gud.fit_transform(X,y)\n","X_new.shape"],"metadata":{"id":"FDf9ymZGYMoE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Wrapper based methods"],"metadata":{"id":"yhV0UJckYhy_"}},{"cell_type":"markdown","source":["RFE [recursive feature elimination]\n","\n","first fit, remove least ranked feature"],"metadata":{"id":"1eCJBV3dZOdT"}},{"cell_type":"code","source":["from sklearn.datasets import make_friendmanl\n","from sklearn.feature_selection import RFE\n","from sklearn.linear_model import LinearRegression\n","\n","estimator = LinearRegression()\n","selector = RFE(esimator, n_features_to_select=3,step=1)\n","selector = selector.fit(X,y)\n","print(selector.support_)\n","print(selector.ranking_)"],"metadata":{"id":"Db02OBmwZjST"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["RFE-CV\n","\n","to add another layer of cross_validation to RFE"],"metadata":{"id":"5UraATaxaDfo"}},{"cell_type":"markdown","source":["SelectFromModel\n","\n","Select desired no of important features above certain threshold of feature importance as obtained from trained estimator."],"metadata":{"id":"I10zAOb-aHiO"}},{"cell_type":"code","source":["from sklearn.feature_selection import SelectFromModel\n","\n","estimator = LinearRegression()\n","estimator.fit(X,y)\n","print(estimator.coef_)\n","print(np.argsort(estimator.coef_)[-3:])\n","t=np.argsort(np.abs(estimator.coef_))[-3:]\n","model = SelectFromModel(estimator, max_features=3,prefit=True)\n","X_new = model.transform(X)\n","print(X_new.shape)"],"metadata":{"id":"utxXSY0Aa1lj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["SequentialFeatureSelection\n","\n","It performs feature selection by selecting or deselecting or deselecting features one by one in a greedy manner."],"metadata":{"id":"p-GqEQebd5v-"}},{"cell_type":"code","source":["from sklearn.feature_selection import SequentialFeatureSelection\n","%%time estimator = LinearRegression()\n","sfs = SequentialFeatureSelection(estimator, n_features_to_select=3)\n","sfs.fit_transform(X,y)\n","sfs.support()"],"metadata":{"id":"n0hXyZqAeEDD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The features corresponding to True in the output"],"metadata":{"id":"ozDYAuvreVe7"}},{"cell_type":"code","source":["%%time estimator = LinearRegression()\n","sfs = SequentialFeatureSelection(estimator, n_features_to_select=3, direction='backward')\n","sfs.fit_transform(X,y)\n","sfs.support()"],"metadata":{"id":"mBrwIvXgecBi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 11. PCA\n","\n","PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that capture maximum amount of variance.\n","\n","It helps in reducing dimensions of a dataset, thus computational cost of next steps, eg training a model, cross validation etc"],"metadata":{"id":"p7EL6HBievFY"}},{"cell_type":"markdown","source":["Let's fit a PCA transformer on this data and compute its two principal components"],"metadata":{"id":"1TCb6gdbfn9o"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","pca = PCA(n_components=2)\n","print(pca.fit(X))\n","print(pca.components_)\n","print(pca.explained_variance_)\n","print(pca.mean_)"],"metadata":{"id":"vNVZCkdtfuqz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Reduced dimensions"],"metadata":{"id":"kZEUIkBChZf_"}},{"cell_type":"code","source":["pca = PCA(n_components=1)\n","pca.fit(X)\n","X_pca = pca.transform(X)\n","print(X.shape,X_pca.shape)"],"metadata":{"id":"b9MdQyxphbqM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 12. Chaining Transformers"],"metadata":{"id":"AArNExrQhs9C"}},{"cell_type":"code","source":["from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler\n","estimators = [('simpleimputer', SimpleImputer()),\n","              ('standardscaler', StandardScaler()),]\n","pipe = Pipeline(steps=estimators)"],"metadata":{"id":"YEdeuCdeh16e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["same can be done via make_pipeline"],"metadata":{"id":"ncEweBPviOlg"}},{"cell_type":"code","source":["from sklearn.pipeline import make_pipeline\n","pipe = make_pipeline(SimpleImputer(),\n","                     StandardScaler())"],"metadata":{"id":"rmwLt-JEiReo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["GridSearch with pipeline\n","\n","by using naming convention of nested parameters, grid search can be implemented"],"metadata":{"id":"AQgYO0j7ifEr"}},{"cell_type":"code","source":["from sklearn.impute import KNNImputer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.model_selection import GridSearchCV\n","\n","param_grid = dict(imputer=['pasthrough',\n","                           SimpleImputer(),\n","                           KNNImputer()],\n","                  clf = [SVC(), LogisticRegression()],\n","                  clf__C=[0.1,10,100])\n","grid_search = GridSearchCV(pipe, param_grid=param_grid)"],"metadata":{"id":"FAV-J7N1jAlQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["c is an inverse of regularization, lower its value stronger the regularization\n","\n","In this example clf__C provides a set of values for grid search."],"metadata":{"id":"rn97CFTqjosm"}},{"cell_type":"markdown","source":["Caching Transformers"],"metadata":{"id":"O97PtZQjjzXW"}},{"cell_type":"markdown","source":["Transforming data is a computationally expensive step.\n","\n","* for grid search, transformers need not be applied for every parameter configuration. They can be applied only once, and the transformed data can be reused."],"metadata":{"id":"lKtxqzyWj7Nr"}},{"cell_type":"code","source":["import tempfile\n","tempDirPath = tempfile.TemporaryDirectory()\n","\n","estimators = [('simpleimputer', SimpleImputer()),\n","              ('pca', PCA()),\n","              ('regressor',LinearRegression())]\n","pipe = Pipeline(steps=estimators, memory=tempDirPath)"],"metadata":{"id":"TNAZkuRSkMtz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["FeatureUnion\n","\n","Concatanates results of multiple transformer objects"],"metadata":{"id":"hOv5bdgHkjOZ"}},{"cell_type":"markdown","source":["# 13. Visualizing Pipelines"],"metadata":{"id":"1oDwfGg_kqRg"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler, LabelBinarizer\n","from sklearn.pipeline import Pipeline, FeatureUnion\n","from sklearn.compose import ColumnTransformer\n","from sklearn.impute import SimpleImputer\n","\n","num_pipeline = Pipeline([('selector', ColumnTransformer([('select_first_4',\n","                                                          'passthrough',\n","                                                          slice(0.4))])),\n","                         ('imputer',SimpleImputer(strategy = 'median')),\n","                         ('std_scaler', StandardScaler()),\n","                         ])\n","cat_pipeline = ColumnTransformer([('label_binarizer', LabelBinarizer(),[4]),])\n","full_pipeline = FeatureUnion(transformer_list = [('num_pipeline',num_pipeline),\n","                                                 ('cat_pipeline',cat_pipeline),\n","                                                 ])"],"metadata":{"id":"zzZ_pMGik2Hd","executionInfo":{"status":"ok","timestamp":1654109410735,"user_tz":-330,"elapsed":887,"user":{"displayName":"RAHUL AGARWAL","userId":"13129226434464566214"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from sklearn import set_config\n","set_config(display='diagram')\n","full_pipeline"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":246},"id":"fRULC2vFmYSN","executionInfo":{"status":"ok","timestamp":1654109426951,"user_tz":-330,"elapsed":8,"user":{"displayName":"RAHUL AGARWAL","userId":"13129226434464566214"}},"outputId":"b6667c9b-4b33-42d3-8af5-51b5266b7954"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["FeatureUnion(transformer_list=[('num_pipeline',\n","                                Pipeline(steps=[('selector',\n","                                                 ColumnTransformer(transformers=[('select_first_4',\n","                                                                                  'passthrough',\n","                                                                                  slice(None, 0.4, None))])),\n","                                                ('imputer',\n","                                                 SimpleImputer(strategy='median')),\n","                                                ('std_scaler',\n","                                                 StandardScaler())])),\n","                               ('cat_pipeline',\n","                                ColumnTransformer(transformers=[('label_binarizer',\n","                                                                 LabelBinarizer(),\n","                                                                 [4])]))])"],"text/html":["<style>#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 {color: black;background-color: white;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 pre{padding: 0;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-toggleable {background-color: white;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-item {z-index: 1;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-parallel-item:only-child::after {width: 0;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-62fe61d1-fcad-4df6-b371-38aba1b429e5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-62fe61d1-fcad-4df6-b371-38aba1b429e5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>FeatureUnion(transformer_list=[(&#x27;num_pipeline&#x27;,\n","                                Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                 ColumnTransformer(transformers=[(&#x27;select_first_4&#x27;,\n","                                                                                  &#x27;passthrough&#x27;,\n","                                                                                  slice(None, 0.4, None))])),\n","                                                (&#x27;imputer&#x27;,\n","                                                 SimpleImputer(strategy=&#x27;median&#x27;)),\n","                                                (&#x27;std_scaler&#x27;,\n","                                                 StandardScaler())])),\n","                               (&#x27;cat_pipeline&#x27;,\n","                                ColumnTransformer(transformers=[(&#x27;label_binarizer&#x27;,\n","                                                                 LabelBinarizer(),\n","                                                                 [4])]))])</pre><b>Please rerun this cell to show the HTML repr or trust the notebook.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"524e67c0-71ab-4050-9411-de4e92a39d16\" type=\"checkbox\" ><label for=\"524e67c0-71ab-4050-9411-de4e92a39d16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FeatureUnion</label><div class=\"sk-toggleable__content\"><pre>FeatureUnion(transformer_list=[(&#x27;num_pipeline&#x27;,\n","                                Pipeline(steps=[(&#x27;selector&#x27;,\n","                                                 ColumnTransformer(transformers=[(&#x27;select_first_4&#x27;,\n","                                                                                  &#x27;passthrough&#x27;,\n","                                                                                  slice(None, 0.4, None))])),\n","                                                (&#x27;imputer&#x27;,\n","                                                 SimpleImputer(strategy=&#x27;median&#x27;)),\n","                                                (&#x27;std_scaler&#x27;,\n","                                                 StandardScaler())])),\n","                               (&#x27;cat_pipeline&#x27;,\n","                                ColumnTransformer(transformers=[(&#x27;label_binarizer&#x27;,\n","                                                                 LabelBinarizer(),\n","                                                                 [4])]))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>num_pipeline</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"23716852-91e3-4556-9d3c-ca705e416d9c\" type=\"checkbox\" ><label for=\"23716852-91e3-4556-9d3c-ca705e416d9c\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">selector: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;select_first_4&#x27;, &#x27;passthrough&#x27;,\n","                                 slice(None, 0.4, None))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"6766c106-0a6c-4254-976d-6f5889b02085\" type=\"checkbox\" ><label for=\"6766c106-0a6c-4254-976d-6f5889b02085\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">select_first_4</label><div class=\"sk-toggleable__content\"><pre>slice(None, 0.4, None)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"711a4f64-0c6b-41d3-9cbc-41e1549c00ce\" type=\"checkbox\" ><label for=\"711a4f64-0c6b-41d3-9cbc-41e1549c00ce\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"bf061658-e38c-40a2-b1c1-3da16fd60f9c\" type=\"checkbox\" ><label for=\"bf061658-e38c-40a2-b1c1-3da16fd60f9c\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"3d3077b7-bd40-4152-8c4a-194bd7a5eb7f\" type=\"checkbox\" ><label for=\"3d3077b7-bd40-4152-8c4a-194bd7a5eb7f\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>cat_pipeline</label></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"ea03a5f0-d68c-4ac4-9a1b-50901e5ad9f0\" type=\"checkbox\" ><label for=\"ea03a5f0-d68c-4ac4-9a1b-50901e5ad9f0\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">label_binarizer</label><div class=\"sk-toggleable__content\"><pre>[4]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"11025a52-93ea-487a-832a-2cb318ff3db1\" type=\"checkbox\" ><label for=\"11025a52-93ea-487a-832a-2cb318ff3db1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelBinarizer</label><div class=\"sk-toggleable__content\"><pre>LabelBinarizer()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["# 14. Handling imbalanced data\n","\n","Imbalanced datasets are those where one class is very less represented than other classes\n","\n","Two main approaches to handle imbalanced data\n","\n","* Undersampling\n","* Oversampling"],"metadata":{"id":"NSoBOTlGm0J9"}},{"cell_type":"code","source":["wine_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv',sep=';')\n","wine_data['quality'].hist(bins=50)\n","plt.xlabel('Quality')\n","plt.ylabel('Number of Samples')\n","plt.show()"],"metadata":{"id":"yIl5P_SfnTJW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Undersampling"],"metadata":{"id":"UiGNN9WXnrUO"}},{"cell_type":"code","source":["from imblearn.under_sampling import RandomSampler\n","\n","class_count_3,class_count_4,class_count_5,class_count_6,class_count_7,class_count_8 = wine_data['quality'].value_counts()\n","\n","class_3 =  wine_data[wine_data['quality']==3]\n","class_4 =  wine_data[wine_data['quality']==4]\n","class_5 =  wine_data[wine_data['quality']==5]\n","class_6 =  wine_data[wine_data['quality']==6]\n","class_7 =  wine_data[wine_data['quality']==7]\n","class_8 =  wine_data[wine_data['quality']==8]\n","\n","print('class 3:', class_3.shape)\n","print('class 4:', class_4.shape)\n","print('class 5:', class_5.shape)\n","print('class 6:', class_6.shape)\n","print('class 7:', class_7.shape)\n","print('class 8:', class_8.shape)\n","\n","from collections import Counter\n","X=wine_data.drop(['quality'], axis=1)\n","y=wine_data['quality']\n","undersample = RandomSampler(random_state=0)\n","X_runs,y_runs = undersample.fit_resample(X,y)\n","print(Counter(y),Counter(y_runs))"],"metadata":{"id":"Ex4Bzfxnn2fl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["OverSampling"],"metadata":{"id":"C553imSOolWX"}},{"cell_type":"code","source":["from imblearn.over_sampling import RandomOverSampler\n","\n","ros = RandomOverSampler()\n","X_ros,y_ros = ros.fit_resample(X,y)\n","print(Counter(y),Counter(y_ros))"],"metadata":{"id":"8R4bth2-pQfw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Types of smote:\n","\n","* Borderline SMOTE\n","* Borderline-SMOTE SVM\n","* Adaptive Synthetic Sampling (ADASYN)"],"metadata":{"id":"8HKdC2c0pmcW"}}]}
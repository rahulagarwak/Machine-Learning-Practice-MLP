{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_Wk3_Baseline_Models.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Baseline Models"
      ],
      "metadata": {
        "id": "KJsjfqOp8mqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will compare our baselines with lin_reg's performance"
      ],
      "metadata": {
        "id": "dDgct-ez8p4L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjaIJXjw8fLJ"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.dummy import DummyRegressor\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import permutation_test_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use ShuffleSplit as a cross validation strategy"
      ],
      "metadata": {
        "id": "1RqqXM349Li6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle_split_cv = ShuffleSplit(n_splits=10, test_size=10,random_state=0)"
      ],
      "metadata": {
        "id": "v26nGbO-9ScO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the data and split into training and test"
      ],
      "metadata": {
        "id": "yTvtAFBU9a3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features, labels = fetch_california_housing(as_frame=True, return_X_y=True)\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(features,labels,random_state=42)"
      ],
      "metadata": {
        "id": "xx-7w2_v9eJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LinearRegression classifier\n",
        "\n",
        "* Build linear regression model with feature scaling as part of a pipeline.\n",
        "* Train the model with 10-fold cross validation via ShuffleSplit\n",
        "* Capture errors on different folds"
      ],
      "metadata": {
        "id": "Pe7qvcoT9sma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lin_reg_pipeline = Pipeline([('feature_scaling',StandardScaler()),\n",
        "                             ('lin_reg',LinearRegression())])\n",
        "\n",
        "lin_reg_score = cross_val_score(lin_reg_pipeline,\n",
        "                                train_features,\n",
        "                                train_labels,\n",
        "                                scoring='neg_mean_squared_error',\n",
        "                                n_jobs=2)\n",
        "lin_reg_errors = pd.Series(-lin_reg_cv_results['test_score'],\n",
        "                           name='Linear regression error')"
      ],
      "metadata": {
        "id": "I68LUJ5W9-Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DummyRegressor"
      ],
      "metadata": {
        "id": "ZVdI_GQ2-gNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dummy_regressor_baseline(strategy, constant_val=None, quantile_val=None):\n",
        "  baseline_model_median = DummyRegressor(strategy=strategy,\n",
        "                                         constant=constant_val,\n",
        "                                         quantile=quantile_val)\n",
        "  baseline_median_cv_results = cross_validate(baseline_model_median,\n",
        "                                              train_features, train_labels,\n",
        "                                              cv=shuffle_split_cv,\n",
        "                                              scoring='neg_mean_absolute_error',\n",
        "                                              n_jobs=2)\n",
        "  return pd.Series(-baseline_median_cv_results['test_score'],name='Dummy regressor error')"
      ],
      "metadata": {
        "id": "7vnUSYMJ-it0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# permutation_test_score\n",
        "\n",
        "It permutes the target to generate randomized data and computes the empirical p-value against the null hypothesis, that features and targets are independent.\n",
        "\n",
        "Here we are interested in permutation_score returned by this API, which indicates score of the model on different permutations."
      ],
      "metadata": {
        "id": "GP0n9dNK_oLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score, permutation_score, pvalue = permutation_test_score(lin_reg_pipeline, train_features, train_labels,\n",
        "                                                          cv=shuffle_split_cv, scoring='neg_mean_absolute_error',n_jobs=2,n_permutations=30)\n",
        "permutation_errors = pd.Series(-permutation_score, name='Permuted error')"
      ],
      "metadata": {
        "id": "h_Qpq6C2AFKl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}